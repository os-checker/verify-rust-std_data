{
  "file": "core/src/ptr/mod.rs",
  "name": "ptr::read::<core_arch::arm_shared::neon::poly64x2_t>",
  "hash": "425313733417334470112760697082345698390",
  "hash_direct": "1159614581899480373417220913530087480158",
  "src": "pub const unsafe fn read<T>(src: *const T) -> T {\n    // It would be semantically correct to implement this via `copy_nonoverlapping`\n    // and `MaybeUninit`, as was done before PR #109035. Calling `assume_init`\n    // provides enough information to know that this is a typed operation.\n\n    // However, as of March 2023 the compiler was not capable of taking advantage\n    // of that information. Thus, the implementation here switched to an intrinsic,\n    // which lowers to `_0 = *src` in MIR, to address a few issues:\n    //\n    // - Using `MaybeUninit::assume_init` after a `copy_nonoverlapping` was not\n    //   turning the untyped copy into a typed load. As such, the generated\n    //   `load` in LLVM didn't get various metadata, such as `!range` (#73258),\n    //   `!nonnull`, and `!noundef`, resulting in poorer optimization.\n    // - Going through the extra local resulted in multiple extra copies, even\n    //   in optimized MIR.  (Ignoring StorageLive/Dead, the intrinsic is one\n    //   MIR statement, while the previous implementation was eight.)  LLVM\n    //   could sometimes optimize them away, but because `read` is at the core\n    //   of so many things, not having them in the first place improves what we\n    //   hand off to the backend.  For example, `mem::replace::<Big>` previously\n    //   emitted 4 `alloca` and 6 `memcpy`s, but is now 1 `alloc` and 3 `memcpy`s.\n    // - In general, this approach keeps us from getting any more bugs (like\n    //   #106369) that boil down to \"`read(p)` is worse than `*p`\", as this\n    //   makes them look identical to the backend (or other MIR consumers).\n    //\n    // Future enhancements to MIR optimizations might well allow this to return\n    // to the previous implementation, rather than using an intrinsic.\n\n    // SAFETY: the caller must guarantee that `src` is valid for reads.\n    unsafe {\n        #[cfg(debug_assertions)] // Too expensive to always enable (for now?)\n        ub_checks::assert_unsafe_precondition!(\n            check_language_ub,\n            \"ptr::read requires that the pointer argument is aligned and non-null\",\n            (\n                addr: *const () = src as *const (),\n                align: usize = align_of::<T>(),\n                is_zst: bool = T::IS_ZST,\n            ) => ub_checks::maybe_is_aligned_and_not_null(addr, align, is_zst)\n        );\n        crate::intrinsics::read_via_copy(src)\n    }\n}",
  "callees": [
    "258820287469578611817448591488600041899",
    "17421822894816687532325615181214565887",
    "27480086118489678191003877857780025576",
    "23815256893174302637797703719499456373",
    "47704708250653028710044748737554169683",
    "1587111310284534591616373149651699419869",
    "37868078735602925891573318122962819320",
    "1578703703635175719018067387082881800385",
    "1113032932815665948217565294476098756403",
    "41505289739977656509611396154325757795",
    "1752989996936949203916036071159051010960",
    "478753423337745817613297159272845606672",
    "3658730781980743391697756456907208995",
    "176386771705132260856587306647620489942",
    "625849434625814717716494369484959093162",
    "1566168013061370513711054548557151263674",
    "187482952606822645712460029589708661523",
    "126818845945534382313624863669169238336",
    "76610361547970390014041401956375253519",
    "739520556846959085018120036998133255835",
    "5533079644241704014802052529604371844",
    "101520258712339119345830936586852421312"
  ]
}
